{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.cm as cmx\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from hmmlearn import hmm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from nilearn import datasets, plotting\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Dimensional Connectome Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data - Separate Modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17906, 68, 68)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_subjects_all_trials_connectomes = utils.load_connectomes(utils.ALL_SUBJECT_IDS, utils.ALL_TRIAL_IDS)\n",
    "all_subjects_all_trials_connectomes['fmri'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract flattened representation of upper triangular of Pearson correlation matrix for each connectome type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: The below logic would have to change if we move away from using Desikan Atlas where the number of regions \n",
    "# are the same between EEG and fMRI\n",
    "num_regions = all_subjects_all_trials_connectomes['fmri'].shape[1]\n",
    "num_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_triangular_including_diagonal_idxs = np.triu_indices(num_regions, k=0)\n",
    "lower_triangular_idxs = np.tril_indices(num_regions, k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects_all_trials_connectome_upper_triangular_flattened = copy.deepcopy(all_subjects_all_trials_connectomes)\n",
    "for k in all_subjects_all_trials_connectome_upper_triangular_flattened:\n",
    "    all_subjects_all_trials_connectome_upper_triangular_flattened[k] = np.array([c[upper_triangular_including_diagonal_idxs].flatten() for c in all_subjects_all_trials_connectomes[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subjects_all_trials_connectome_upper_triangular_flattened['fmri'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data - Combined Modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all modalities into a single time series by smushing feature vectors together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_per_modality = all_subjects_all_trials_connectome_upper_triangular_flattened['fmri'].shape[1]\n",
    "fmri_feature_idxs  = range(num_features_per_modality*0, num_features_per_modality*1)\n",
    "alpha_feature_idxs = range(num_features_per_modality*1, num_features_per_modality*2)\n",
    "beta_feature_idxs  = range(num_features_per_modality*2, num_features_per_modality*3)\n",
    "delta_feature_idxs = range(num_features_per_modality*3, num_features_per_modality*4)\n",
    "gamma_feature_idxs = range(num_features_per_modality*4, num_features_per_modality*5)\n",
    "theta_feature_idxs = range(num_features_per_modality*5, num_features_per_modality*6)\n",
    "\n",
    "feature_idxs = [('fmri', fmri_feature_idxs),\n",
    "                ('alpha', alpha_feature_idxs),\n",
    "                ('beta', beta_feature_idxs),\n",
    "                ('delta', delta_feature_idxs),\n",
    "                ('gamma', gamma_feature_idxs),\n",
    "                ('theta', theta_feature_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_modality_all_subjects_all_trials_connectome_upper_triangular_flattened = []\n",
    "for k in all_subjects_all_trials_connectome_upper_triangular_flattened:\n",
    "    all_modality_all_subjects_all_trials_connectome_upper_triangular_flattened.append(all_subjects_all_trials_connectome_upper_triangular_flattened[k])\n",
    "all_modality_all_subjects_all_trials_connectome_upper_triangular_flattened = np.concatenate(all_modality_all_subjects_all_trials_connectome_upper_triangular_flattened, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17906, 14076)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_modality_all_subjects_all_trials_connectome_upper_triangular_flattened.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17906, 2346)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_modality_all_subjects_all_trials_connectome_upper_triangular_flattened[:, fmri_feature_idxs].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) TODO: BIC and AIC as a normalization scheme\n",
    "\n",
    "(2) TODO: PCA first then project back out through PCs\n",
    "\n",
    "(3) TODO: Train HMMs on subset of data and predict likelihood of new data\n",
    "\n",
    "(4) TODO: Find a bunch of graph statistics (node-wise or global) and compute the time series of the graph statistics and fit an HMM to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_optimal_hmm_on_data(data, bic=True, pca_variance_retained=0.99, forced_component_count=None):\n",
    "    \n",
    "#     # Apply PCA to dimensionality reduce the data and retain xx% variance\n",
    "#     print(\"\\tApplying pca and retaining {0:.2f}% variance...\".format(pca_variance_retained*100))\n",
    "#     pca_model = PCA(pca_variance_retained)\n",
    "#     dim_reduced_data = pca_model.fit_transform(data)\n",
    "#     hmm_preprocessing_pca_models.append(pca_model)\n",
    "#     print(\"\\t{0} dimensions -> {1} dimensions\".format(data.shape[1], dim_reduced_data.shape[1]))\n",
    "    \n",
    "#     # Find HMM model with lowest BIC/AIC score with binary search\n",
    "#     bottom_n_components = 2\n",
    "#     top_n_components = min(100, dim_reduced_data.shape[0]-1)\n",
    "    \n",
    "#     while bottom_n_components != top_n_components:\n",
    "        \n",
    "#         # Select mid of component range\n",
    "#         print(\"\\t\\t Searching for num components in hyperparameter range: {0}-{1}\".format(bottom_n_components, top_n_components))\n",
    "#         n_components = (top_n_components+bottom_n_components)//2\n",
    "        \n",
    "#         # Check end condition\n",
    "#         if bottom_n_components == top_n_components:\n",
    "#             print(\"\\t\\tFound optimal number of components: {0}\".format(n_components))\n",
    "#             return hmm.GaussianHMM(n_components=n_components,\n",
    "#                                    covariance_type=\"diag\").fit(dim_reduced_data)\n",
    "\n",
    "#         # Train HMM model\n",
    "#         print(\"\\t\\t training hmm with n_components={0}\".format(n_components))\n",
    "#         candidate_hmm_model = hmm.GaussianHMM(n_components=n_components,\n",
    "#                                               covariance_type=\"diag\").fit(dim_reduced_data)\n",
    "#         print(\"\\t\\t training hmm with n_components={0}\".format(n_components-1))\n",
    "#         candidate_hmm_model_1 = hmm.GaussianHMM(n_components=n_components-1,\n",
    "#                                                covariance_type=\"diag\").fit(dim_reduced_data)\n",
    "#         # Compute BIC/AIC score\n",
    "#         n = dim_reduced_data.shape[0]\n",
    "#         k = num_parameters_in_hmm(candidate_hmm_model)\n",
    "#         L = candidate_hmm_model.decode(dim_reduced_data)[0]\n",
    "#         score = (np.log(n)*k - 2*L) if bic else (k - L)\n",
    "\n",
    "#         print(\"\\t\\t\\tn={0}, k={1}, L={2}\".format(n, k, L))\n",
    "#         print(\"\\t\\t\\tScore={0}\".format(score))\n",
    "        \n",
    "#         k_1 = num_parameters_in_hmm(candidate_hmm_model_1)\n",
    "#         L_1 = candidate_hmm_model_1.decode(dim_reduced_data)[0]\n",
    "#         score_1 = (np.log(n)*k_1 - 2*L_1) if bic else (k_1 - L_1)\n",
    "\n",
    "#         print(\"\\t\\t\\tn={0}, k_1={1}, L_1={2}\".format(n, k_1, L_1))\n",
    "#         print(\"\\t\\t\\tScore_1={0}\".format(score_1))\n",
    "    \n",
    "#         if score_1 <= score:\n",
    "#             top_n_components = n_components\n",
    "#         else:\n",
    "#             bottom_n_components = n_components\n",
    "\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_in_hmm(model):  \n",
    "    k = model.means_.shape[0]\n",
    "    d = model.means_.shape[1]\n",
    "    # NOTE: We are using a diagonal covariance matrix\n",
    "    return (k*d)+(k*d)+k+(k**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optimal_hmm_on_data(data, bic=True, pca_variance_retained=0.99, forced_component_count=None):\n",
    "    \n",
    "    # Apply PCA to dimensionality reduce the data and retain xx% variance\n",
    "    print(\"\\tApplying pca and retaining {0:.2f}% variance...\".format(pca_variance_retained*100))\n",
    "    pca_model = PCA(pca_variance_retained)\n",
    "    dim_reduced_data = pca_model.fit_transform(data)\n",
    "    hmm_preprocessing_pca_models.append(pca_model)\n",
    "    print(\"\\t{0} dimensions -> {1} dimensions\".format(data.shape[1], dim_reduced_data.shape[1]))\n",
    "    \n",
    "    # Compute BIC/AIC scores for all hmm models with num_states between 2 and 50\n",
    "    scores = []\n",
    "    candidate_hmm_models = []\n",
    "    \n",
    "    for n_components in (range(1, data.shape[0]-1) if not forced_component_count else range(forced_component_count, forced_component_count+1)):\n",
    "        \n",
    "        print(\"\\t\\t training hmm with n_components={0}\".format(n_components))\n",
    "\n",
    "        # Train HMM model\n",
    "        candidate_hmm_model = hmm.GaussianHMM(n_components=n_components,\n",
    "                                              covariance_type=\"diag\").fit(dim_reduced_data)\n",
    "        candidate_hmm_models.append(candidate_hmm_model)\n",
    "\n",
    "        # Compute BIC/AIC score\n",
    "        n = dim_reduced_data.shape[0]\n",
    "        k = num_parameters_in_hmm(candidate_hmm_model)\n",
    "        L = candidate_hmm_model.decode(dim_reduced_data)[0]\n",
    "\n",
    "        print(\"\\t\\t\\tn={0}, k={1}, L={2}\".format(n, k, L))\n",
    "\n",
    "        score = (np.log(n)*k - 2*L) if bic else (k - L)\n",
    "        scores.append(score)\n",
    "        print(\"\\t\\t\\tScore={0}\".format(score))\n",
    "        \n",
    "        if len(scores) >= 3:\n",
    "            # Last two increases in number of components yielded worse scores -> exit search\n",
    "            if scores[-1] > scores[-2] and scores[-2] > scores[-3]:\n",
    "                break\n",
    "    \n",
    "    # Select the model with the lowest BIC/AIC score\n",
    "    selected_hmm_model = candidate_hmm_models[np.argmin(scores)]\n",
    "    return selected_hmm_model, pca_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train HMMs separately on data from each modality using the Bayesian Information Criterion (BIC) to select the number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fmri\n",
      "\tApplying pca and retaining 90.00% variance...\n",
      "\t2346 dimensions -> 470 dimensions\n",
      "\t\t training hmm with n_components=1\n",
      "\t\t\tn=17906, k=942, L=-7513945.386984726\n",
      "\t\t\tScore=15037115.677415038\n",
      "\t\t training hmm with n_components=2\n",
      "\t\t\tn=17906, k=1886, L=-7458867.1515939785\n",
      "\t\t\tScore=14936203.69586139\n",
      "\t\t training hmm with n_components=3\n",
      "\t\t\tn=17906, k=2832, L=-7439867.45385991\n",
      "\t\t\tScore=14907468.375403363\n",
      "\t\t training hmm with n_components=4\n",
      "\t\t\tn=17906, k=3780, L=-7426174.119843681\n",
      "\t\t\tScore=14889365.368163276\n",
      "\t\t training hmm with n_components=5\n",
      "\t\t\tn=17906, k=4730, L=-7414373.305099972\n",
      "\t\t\tScore=14875066.985250492\n",
      "\t\t training hmm with n_components=6\n",
      "\t\t\tn=17906, k=5682, L=-7404173.5841353405\n",
      "\t\t\tScore=14863990.375678126\n",
      "\t\t training hmm with n_components=7\n",
      "\t\t\tn=17906, k=6636, L=-7391452.563198404\n",
      "\t\t\tScore=14847890.751943413\n",
      "\t\t training hmm with n_components=8\n",
      "\t\t\tn=17906, k=7592, L=-7378460.476156975\n",
      "\t\t\tScore=14831268.581781976\n",
      "\t\t training hmm with n_components=9\n",
      "\t\t\tn=17906, k=8550, L=-7364270.124106195\n",
      "\t\t\tScore=14812269.4673841\n",
      "\t\t training hmm with n_components=10\n",
      "\t\t\tn=17906, k=9510, L=-7357360.068542456\n",
      "\t\t\tScore=14807850.531742569\n",
      "\t\t training hmm with n_components=11\n",
      "\t\t\tn=17906, k=10472, L=-7347798.672164382\n",
      "\t\t\tScore=14798148.50025463\n",
      "\t\t training hmm with n_components=12\n",
      "\t\t\tn=17906, k=11436, L=-7334608.653320642\n",
      "\t\t\tScore=14781208.80961762\n",
      "\t\t training hmm with n_components=13\n",
      "\t\t\tn=17906, k=12402, L=-7323996.706662269\n",
      "\t\t\tScore=14769444.849133607\n",
      "\t\t training hmm with n_components=14\n",
      "\t\t\tn=17906, k=13370, L=-7311057.868396691\n",
      "\t\t\tScore=14753046.691217447\n",
      "\t\t training hmm with n_components=15\n",
      "\t\t\tn=17906, k=14340, L=-7302343.705847295\n",
      "\t\t\tScore=14745117.470515914\n",
      "\t\t training hmm with n_components=16\n",
      "\t\t\tn=17906, k=15312, L=-7285841.021719601\n",
      "\t\t\tScore=14721630.792440046\n",
      "\t\t training hmm with n_components=17\n",
      "\t\t\tn=17906, k=16286, L=-7266984.1169384485\n",
      "\t\t\tScore=14693455.258839523\n",
      "\t\t training hmm with n_components=18\n",
      "\t\t\tn=17906, k=17262, L=-7258944.22867257\n",
      "\t\t\tScore=14686933.344051812\n",
      "\t\t training hmm with n_components=19\n",
      "\t\t\tn=17906, k=18240, L=-7253045.778010683\n",
      "\t\t\tScore=14684713.890254347\n",
      "\t\t training hmm with n_components=20\n",
      "\t\t\tn=17906, k=19220, L=-7232323.797532735\n",
      "\t\t\tScore=14652866.96260702\n",
      "\t\t training hmm with n_components=21\n",
      "\t\t\tn=17906, k=20202, L=-7223529.822834604\n",
      "\t\t\tScore=14644895.632301591\n",
      "\t\t training hmm with n_components=22\n",
      "\t\t\tn=17906, k=21186, L=-7211113.689907878\n",
      "\t\t\tScore=14629699.571321234\n",
      "\t\t training hmm with n_components=23\n",
      "\t\t\tn=17906, k=22172, L=-7194603.322041591\n",
      "\t\t\tScore=14606334.626244018\n",
      "\t\t training hmm with n_components=24\n",
      "\t\t\tn=17906, k=23160, L=-7182712.489048803\n",
      "\t\t\tScore=14592228.336696062\n",
      "\t\t training hmm with n_components=25\n",
      "\t\t\tn=17906, k=24150, L=-7177409.039651814\n",
      "\t\t\tScore=14591316.400121965\n",
      "\t\t training hmm with n_components=26\n",
      "\t\t\tn=17906, k=25142, L=-7164733.570682689\n",
      "\t\t\tScore=14575680.01018586\n",
      "\t\t training hmm with n_components=27\n",
      "\t\t\tn=17906, k=26136, L=-7146356.931832795\n",
      "\t\t\tScore=14548660.866270479\n",
      "\t\t training hmm with n_components=28\n",
      "\t\t\tn=17906, k=27132, L=-7140819.97908046\n",
      "\t\t\tScore=14547340.680332478\n",
      "\t\t training hmm with n_components=29\n",
      "\t\t\tn=17906, k=28130, L=-7120316.49505932\n",
      "\t\t\tScore=14516107.01763913\n",
      "\t\t training hmm with n_components=30\n",
      "\t\t\tn=17906, k=29130, L=-7107938.416451695\n",
      "\t\t\tScore=14501143.751555074\n",
      "\t\t training hmm with n_components=31\n",
      "\t\t\tn=17906, k=30132, L=-7094185.085802153\n",
      "\t\t\tScore=14483449.567169446\n",
      "\t\t training hmm with n_components=32\n",
      "\t\t\tn=17906, k=31136, L=-7081454.991887215\n",
      "\t\t\tScore=14467821.44203529\n",
      "\t\t training hmm with n_components=33\n",
      "\t\t\tn=17906, k=32142, L=-7066379.166836524\n",
      "\t\t\tScore=14447521.44041189\n",
      "\t\t training hmm with n_components=34\n",
      "\t\t\tn=17906, k=33150, L=-7060900.265315613\n",
      "\t\t\tScore=14446434.87163031\n",
      "\t\t training hmm with n_components=35\n",
      "\t\t\tn=17906, k=34160, L=-7048056.493976214\n",
      "\t\t\tScore=14430638.14899402\n",
      "\t\t training hmm with n_components=36\n",
      "\t\t\tn=17906, k=35172, L=-7032034.348358159\n",
      "\t\t\tScore=14408504.263582677\n",
      "\t\t training hmm with n_components=37\n",
      "\t\t\tn=17906, k=36186, L=-7022012.334605848\n",
      "\t\t\tScore=14398390.227685086\n",
      "\t\t training hmm with n_components=38\n",
      "\t\t\tn=17906, k=37202, L=-7002723.9808210805\n",
      "\t\t\tScore=14369763.097504845\n",
      "\t\t training hmm with n_components=39\n",
      "\t\t\tn=17906, k=38220, L=-6992840.2404836\n",
      "\t\t\tScore=14359964.780001437\n",
      "\t\t training hmm with n_components=40\n",
      "\t\t\tn=17906, k=39240, L=-6994040.1204175465\n",
      "\t\t\tScore=14372353.28882315\n",
      "\t\t training hmm with n_components=41\n",
      "\t\t\tn=17906, k=40262, L=-6969330.067349651\n",
      "\t\t\tScore=14332941.517423438\n",
      "\t\t training hmm with n_components=42\n",
      "\t\t\tn=17906, k=41286, L=-6954145.978784073\n",
      "\t\t\tScore=14312601.260810627\n",
      "\t\t training hmm with n_components=43\n",
      "\t\t\tn=17906, k=42312, L=-6950335.599460715\n",
      "\t\t\tScore=14315028.008464515\n",
      "\t\t training hmm with n_components=44\n",
      "\t\t\tn=17906, k=43340, L=-6931891.2780953245\n",
      "\t\t\tScore=14288206.4578166\n",
      "\t\t training hmm with n_components=45\n",
      "\t\t\tn=17906, k=44370, L=-6913425.483755623\n",
      "\t\t\tScore=14261361.547002329\n",
      "\t\t training hmm with n_components=46\n",
      "\t\t\tn=17906, k=45402, L=-6910500.869090189\n",
      "\t\t\tScore=14265618.581318853\n",
      "\t\t training hmm with n_components=47\n",
      "\t\t\tn=17906, k=46436, L=-6889331.933805862\n",
      "\t\t\tScore=14233406.560179854\n",
      "\t\t training hmm with n_components=48\n",
      "\t\t\tn=17906, k=47472, L=-6883392.8278500335\n",
      "\t\t\tScore=14231673.783480113\n",
      "\t\t training hmm with n_components=49\n",
      "\t\t\tn=17906, k=48510, L=-6873136.935110698\n",
      "\t\t\tScore=14221327.018995622\n",
      "\t\t training hmm with n_components=50\n",
      "\t\t\tn=17906, k=49550, L=-6862387.689065578\n",
      "\t\t\tScore=14210013.133681824\n",
      "\t\t training hmm with n_components=51\n",
      "\t\t\tn=17906, k=50592, L=-6853564.4784405725\n",
      "\t\t\tScore=14202570.904990517\n",
      "\t\t training hmm with n_components=52\n",
      "\t\t\tn=17906, k=51636, L=-6842880.37425673\n",
      "\t\t\tScore=14191426.4749638\n",
      "\t\t training hmm with n_components=53\n",
      "\t\t\tn=17906, k=52682, L=-6817146.182318055\n",
      "\t\t\tScore=14150201.455209678\n",
      "\t\t training hmm with n_components=54\n",
      "\t\t\tn=17906, k=53730, L=-6808916.426149445\n",
      "\t\t\tScore=14144004.892777948\n",
      "\t\t training hmm with n_components=55\n",
      "\t\t\tn=17906, k=54780, L=-6800214.065825565\n",
      "\t\t\tScore=14136882.707817942\n",
      "\t\t training hmm with n_components=56\n",
      "\t\t\tn=17906, k=55832, L=-6776735.335079465\n",
      "\t\t\tScore=14100227.36779576\n",
      "\t\t training hmm with n_components=57\n",
      "\t\t\tn=17906, k=56886, L=-6766427.653264476\n",
      "\t\t\tScore=14089933.711418059\n",
      "\t\t training hmm with n_components=58\n",
      "\t\t\tn=17906, k=57942, L=-6756849.03103876\n",
      "\t\t\tScore=14081117.76000117\n",
      "\t\t training hmm with n_components=59\n",
      "\t\t\tn=17906, k=59000, L=-6749749.42309749\n",
      "\t\t\tScore=14077279.422935432\n",
      "\t\t training hmm with n_components=60\n",
      "\t\t\tn=17906, k=60060, L=-6723717.255810271\n",
      "\t\t\tScore=14035595.55296006\n",
      "\t\t training hmm with n_components=61\n",
      "\t\t\tn=17906, k=61122, L=-6710340.4346328685\n",
      "\t\t\tScore=14019241.960986583\n",
      "\t\t training hmm with n_components=62\n",
      "\t\t\tn=17906, k=62186, L=-6700421.7431376325\n",
      "\t\t\tScore=14009824.214159701\n",
      "\t\t training hmm with n_components=63\n",
      "\t\t\tn=17906, k=63252, L=-6687724.194261709\n",
      "\t\t\tScore=13994868.338353708\n",
      "\t\t training hmm with n_components=64\n",
      "\t\t\tn=17906, k=64320, L=-6682165.8484736765\n",
      "\t\t\tScore=13994210.454505758\n",
      "\t\t training hmm with n_components=65\n",
      "\t\t\tn=17906, k=65390, L=-6683296.100545723\n",
      "\t\t\tScore=14006949.352160228\n",
      "\t\t training hmm with n_components=66\n",
      "\t\t\tn=17906, k=66462, L=-6652364.64944799\n",
      "\t\t\tScore=13955584.429257402\n",
      "\t\t training hmm with n_components=67\n",
      "\t\t\tn=17906, k=67536, L=-6652090.918882249\n",
      "\t\t\tScore=13965554.533200823\n",
      "\t\t training hmm with n_components=68\n",
      "\t\t\tn=17906, k=68612, L=-6637412.67039413\n",
      "\t\t\tScore=13946735.18708175\n",
      "\t\t training hmm with n_components=69\n",
      "\t\t\tn=17906, k=69690, L=-6621844.67848969\n",
      "\t\t\tScore=13926155.939912297\n",
      "\t\t training hmm with n_components=70\n",
      "\t\t\tn=17906, k=70770, L=-6602263.5308521\n",
      "\t\t\tScore=13897569.967058806\n",
      "\t\t training hmm with n_components=71\n",
      "\t\t\tn=17906, k=71852, L=-6596204.455590138\n",
      "\t\t\tScore=13896047.724738834\n",
      "\t\t training hmm with n_components=72\n",
      "\t\t\tn=17906, k=72936, L=-6581725.016506823\n",
      "\t\t\tScore=13877704.340558419\n",
      "\t\t training hmm with n_components=73\n",
      "\t\t\tn=17906, k=74022, L=-6565009.071667328\n",
      "\t\t\tScore=13854907.530647906\n",
      "\t\t training hmm with n_components=74\n",
      "\t\t\tn=17906, k=75110, L=-6553911.575794305\n",
      "\t\t\tScore=13843367.204452598\n",
      "\t\t training hmm with n_components=75\n",
      "\t\t\tn=17906, k=76200, L=-6540695.638776227\n",
      "\t\t\tScore=13827609.581749445\n",
      "\t\t training hmm with n_components=76\n",
      "\t\t\tn=17906, k=77292, L=-6530181.663422484\n",
      "\t\t\tScore=13817275.468157222\n",
      "\t\t training hmm with n_components=77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tn=17906, k=78386, L=-6527161.449187555\n",
      "\t\t\tScore=13821948.462584892\n",
      "\t\t training hmm with n_components=78\n",
      "\t\t\tn=17906, k=79482, L=-6496599.387513669\n",
      "\t\t\tScore=13771557.347916909\n",
      "\t\t training hmm with n_components=79\n",
      "\t\t\tn=17906, k=80580, L=-6500294.509535411\n",
      "\t\t\tScore=13789700.186422443\n",
      "\t\t training hmm with n_components=80\n",
      "\t\t\tn=17906, k=81680, L=-6482192.4563910635\n",
      "\t\t\tScore=13764268.26037806\n",
      "\t\t training hmm with n_components=81\n",
      "\t\t\tn=17906, k=82782, L=-6454319.799109482\n",
      "\t\t\tScore=13719314.711841475\n",
      "\t\t training hmm with n_components=82\n",
      "\t\t\tn=17906, k=83886, L=-6455111.952175117\n",
      "\t\t\tScore=13731710.369781584\n",
      "\t\t training hmm with n_components=83\n",
      "\t\t\tn=17906, k=84992, L=-6448903.122048723\n",
      "\t\t\tScore=13730123.647119895\n",
      "\t\t training hmm with n_components=84\n",
      "\t\t\tn=17906, k=86100, L=-6436014.463989821\n",
      "\t\t\tScore=13715196.854375456\n",
      "\t\t training hmm with n_components=85\n",
      "\t\t\tn=17906, k=87210, L=-6419530.779313029\n",
      "\t\t\tScore=13693099.594177496\n",
      "\t\t training hmm with n_components=86\n",
      "\t\t\tn=17906, k=88322, L=-6405370.536167777\n",
      "\t\t\tScore=13675668.802824879\n",
      "\t\t training hmm with n_components=87\n",
      "\t\t\tn=17906, k=89436, L=-6400421.228646789\n",
      "\t\t\tScore=13676679.468503052\n",
      "\t\t training hmm with n_components=88\n",
      "\t\t\tn=17906, k=90552, L=-6379960.233357629\n",
      "\t\t\tScore=13646686.344427146\n",
      "\t\t training hmm with n_components=89\n",
      "\t\t\tn=17906, k=91670, L=-6367130.157944276\n",
      "\t\t\tScore=13631974.645885115\n",
      "\t\t training hmm with n_components=90\n",
      "\t\t\tn=17906, k=92790, L=-6355805.385798103\n",
      "\t\t\tScore=13620293.139659707\n",
      "\t\t training hmm with n_components=91\n",
      "\t\t\tn=17906, k=93912, L=-6354554.08662081\n",
      "\t\t\tScore=13628778.165154321\n",
      "\t\t training hmm with n_components=92\n",
      "\t\t\tn=17906, k=95036, L=-6319774.188713563\n",
      "\t\t\tScore=13570225.578971287\n",
      "\t\t training hmm with n_components=93\n",
      "\t\t\tn=17906, k=96162, L=-6316793.932779551\n",
      "\t\t\tScore=13575291.862516988\n",
      "\t\t training hmm with n_components=94\n",
      "\t\t\tn=17906, k=97290, L=-6299300.38596681\n",
      "\t\t\tScore=13551351.150087494\n",
      "\t\t training hmm with n_components=95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-03f87e386efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_subjects_all_trials_connectome_upper_triangular_flattened\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mhmm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_optimal_hmm_on_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_variance_retained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mhmm_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmm_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mhmm_preprocessing_pca_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-bb8f91e3f23d>\u001b[0m in \u001b[0;36mtrain_optimal_hmm_on_data\u001b[0;34m(data, bic, pca_variance_retained, forced_component_count)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Train HMM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         candidate_hmm_model = hmm.GaussianHMM(n_components=n_components,\n\u001b[0;32m---> 20\u001b[0;31m                                               covariance_type=\"diag\").fit(dim_reduced_data)\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mcandidate_hmm_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_hmm_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blisschapman/Library/Python/3.6/lib/python/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 self._accumulate_sufficient_statistics(\n\u001b[1;32m    438\u001b[0m                     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframelogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposteriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfwdlattice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                     bwdlattice)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;31m# XXX must be before convergence check, because otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blisschapman/Library/Python/3.6/lib/python/site-packages/hmmlearn/hmm.py\u001b[0m in \u001b[0;36m_accumulate_sufficient_statistics\u001b[0;34m(self, stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    226\u001b[0m                                           posteriors, fwdlattice, bwdlattice):\n\u001b[1;32m    227\u001b[0m         super(GaussianHMM, self)._accumulate_sufficient_statistics(\n\u001b[0;32m--> 228\u001b[0;31m             stats, obs, framelogprob, posteriors, fwdlattice, bwdlattice)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'm'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'c'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/blisschapman/Library/Python/3.6/lib/python/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36m_accumulate_sufficient_statistics\u001b[0;34m(self, stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    629\u001b[0m                                       \u001b[0mlog_mask_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                                       \u001b[0mbwdlattice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframelogprob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m                                       log_xi_sum)\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trans'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_xi_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hmm_preprocessing_pca_models = []\n",
    "hmm_models = []\n",
    "\n",
    "for k in all_subjects_all_trials_connectome_upper_triangular_flattened:\n",
    "    \n",
    "    print(k)\n",
    "    data = all_subjects_all_trials_connectome_upper_triangular_flattened[k]\n",
    "    hmm_model, pca_model = train_optimal_hmm_on_data(data, pca_variance_retained=0.90)\n",
    "    hmm_models.append(hmm_model)\n",
    "    hmm_preprocessing_pca_models.append(pca_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a single HMM on data from all modalities combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tApplying pca and retaining 90.00% variance...\n",
      "\t14076 dimensions -> 2096 dimensions\n",
      "\t\t training hmm with n_components=10\n",
      "\t\t\tn=17906, k=43953230, L=2382718.800805331\n",
      "\t\t\tScore=425663758.65272385\n"
     ]
    }
   ],
   "source": [
    "combined_modality_hmm_model, combined_modality_preprocessing_pca_model = train_optimal_hmm_on_data(all_modality_all_subjects_all_trials_connectome_upper_triangular_flattened, pca_variance_retained=0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Representation of Hidden States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPARATE MODALTIES - Plot spatial representation of each hidden markov model state in connectome space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 470)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_models[0].means_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(470, 2346)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_preprocessing_pca_models[0].components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_modality_hmm_model, combined_modality_preprocessing_pca_model = combined_modality_hmm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(180, 60))\n",
    "fig.suptitle('Spatial Loadings of Hidden Markov Model States', fontsize=40)\n",
    "\n",
    "subplot_idx = 1\n",
    "for (k, hmm_model, pca_model) in zip(all_subjects_all_trials_connectome_upper_triangular_flattened, hmm_models, hmm_preprocessing_pca_models):\n",
    "    for component_idx in range(0, n_components):\n",
    "        \n",
    "        # Extract connectome representation of the hidden state\n",
    "        hidden_state = np.zeros((num_regions, num_regions))\n",
    "        hidden_state[upper_triangular_including_diagonal_idxs] = hmm_model.means_[component_idx]\n",
    "        hidden_state[lower_triangular_idxs] = hidden_state.T[lower_triangular_idxs]\n",
    "\n",
    "        # Plot connectome representation of the hidden state\n",
    "        ax = fig.add_subplot(len(hmm_models), n_components, subplot_idx)\n",
    "        plotting.plot_connectome(hidden_state, desikan_atlas_coordinates(), title='{0} HiddenState-{1} Connectome'.format(k, component_idx+1),\n",
    "                                 edge_threshold='95%', node_size=20, colorbar=True, axes=ax)\n",
    "        subplot_idx += 1\n",
    "\n",
    "plt.savefig('output/hmm/spatial_loadings.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINED MODALITIES - Plot spatial representation of each hidden markov model state in connectome space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_modality_hmm_model.means_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(180, 60))\n",
    "fig.suptitle('Spatial Loadings of Hidden Markov Model States', fontsize=40)\n",
    "\n",
    "subplot_idx = 1\n",
    "for feature_desc, feature_idx in feature_idxs:\n",
    "    for component_idx in range(0, n_components):\n",
    "        \n",
    "        # Extract connectome representation of the hidden state\n",
    "        hidden_state = np.zeros((num_regions, num_regions))\n",
    "        hidden_state[upper_triangular_including_diagonal_idxs] = combined_modality_hmm_model.means_[component_idx][feature_idx]\n",
    "        hidden_state[lower_triangular_idxs] = hidden_state.T[lower_triangular_idxs]\n",
    "\n",
    "        # Plot connectome representation of the hidden state\n",
    "        ax = fig.add_subplot(len(feature_idxs), n_components, subplot_idx)\n",
    "        plotting.plot_connectome(hidden_state, desikan_atlas_coordinates(), title='{0} HiddenState-{1} Connectome'.format(feature_desc, component_idx+1),\n",
    "                                 edge_threshold='95%', node_size=20, colorbar=True, axes=ax)\n",
    "        subplot_idx += 1\n",
    "\n",
    "plt.savefig('output/hmm/combined_modality_spatial_loadings.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoded Hidden State Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPARATE MODALITIES - Compute decoded state sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_state_sequences = []\n",
    "decoded_state_sequences_log_likelihoods = []\n",
    "for (k, hmm_model) in zip(all_subjects_all_trials_connectome_upper_triangular_flattened, hmm_models):\n",
    "    decoded_state_sequence = hmm_model.decode(all_subjects_all_trials_connectome_upper_triangular_flattened[k])\n",
    "    decoded_state_sequences_log_likelihoods.append(decoded_state_sequence[0])\n",
    "    decoded_state_sequences.append(decoded_state_sequence[1])\n",
    "    \n",
    "print(decoded_state_sequences_log_likelihoods)\n",
    "print(sum(decoded_state_sequences_log_likelihoods)/len(decoded_state_sequences_log_likelihoods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINED MODALITIES - Compute decoded state sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_modality_log_likelihood, combined_modality_decoded_state_sequence = combined_modality_hmm_model.decode(all_modality_all_subjects_all_trials_connectome_upper_triangular_flattened)\n",
    "print(combined_modality_log_likelihood/6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPARATE MODALITIES - Compute decoded state sequence statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fractional Occupancy - the fraction of time spent in each state relative to the total duration\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "fig.suptitle('Fractional Occupancy')\n",
    "num_plots = len(decoded_state_sequences)\n",
    "subplot_idx = 1\n",
    "\n",
    "for (k, state_seq) in zip(all_subjects_all_trials_connectome_upper_triangular_flattened, decoded_state_sequences):\n",
    "    \n",
    "    # Compute fractional occupancy\n",
    "    fractional_occupancies_per_state = []\n",
    "    for state_idx in range(0, n_components):\n",
    "        fractional_occupancy = len(state_seq[state_seq == state_idx])/len(state_seq)\n",
    "        fractional_occupancies_per_state.append(fractional_occupancy)\n",
    "        \n",
    "    # Plot fractional occupancy per state\n",
    "    fig.add_subplot(1, num_plots, subplot_idx)\n",
    "    plt.title(k)\n",
    "    x = np.arange(n_components)\n",
    "    plt.bar(x, height=fractional_occupancies_per_state)\n",
    "    plt.xticks(x, [str(x_i) for x_i in x])\n",
    "    subplot_idx += 1\n",
    "    \n",
    "plt.savefig('output/hmm/fractional_occupancy.png')\n",
    "fig.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Life Time - the time spent in a state before transitioning to a new state on average\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "fig.suptitle('Mean Life Time')\n",
    "num_plots = len(decoded_state_sequences)\n",
    "subplot_idx = 1\n",
    "\n",
    "for (k, state_seq) in zip(all_subjects_all_trials_connectome_upper_triangular_flattened, decoded_state_sequences):\n",
    "\n",
    "    # Compute mean life time per state\n",
    "    mean_life_time_per_state = []\n",
    "    for state_id in range(0, n_components):\n",
    "        \n",
    "        # Count number of transitions out of state with state_id\n",
    "        num_transitions_out_of_state_with_state_id = 0\n",
    "        for i in range(0, len(state_seq)-1):\n",
    "            if state_seq[i] == state_id and state_seq[i+1] != state_id:\n",
    "                num_transitions_out_of_state_with_state_id += 1\n",
    "        \n",
    "        # Count total number of time points spent in state with state id\n",
    "        num_time_points_in_state_with_state_id = len(state_seq[state_seq == state_id])\n",
    "        \n",
    "        # Compute mean life time\n",
    "        mean_life_time = num_time_points_in_state_with_state_id/num_transitions_out_of_state_with_state_id\n",
    "        mean_life_time_per_state.append(mean_life_time)\n",
    "    \n",
    "    \n",
    "    # Plot mean life time per state\n",
    "    fig.add_subplot(1, num_plots, subplot_idx)\n",
    "    plt.title(k)\n",
    "    x = np.arange(n_components)\n",
    "    plt.bar(x, height=mean_life_time_per_state)\n",
    "    plt.xticks(x, [str(x_i) for x_i in x])\n",
    "    subplot_idx += 1\n",
    "    \n",
    "plt.savefig('output/hmm/mean_life_time.png')\n",
    "fig.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 4))\n",
    "fig.suptitle('Transition Probabilities')\n",
    "num_plots = len(hmm_models)\n",
    "subplot_idx = 1\n",
    "\n",
    "for (k, hmm_model) in zip(all_subjects_all_trials_connectome_upper_triangular_flattened, hmm_models):\n",
    "    \n",
    "    fig.add_subplot(1, num_plots, subplot_idx)\n",
    "    plt.imshow(hmm_model.transmat_, cmap='gist_heat')\n",
    "    plt.title(k)\n",
    "    plt.colorbar()\n",
    "    subplot_idx += 1\n",
    "\n",
    "plt.savefig('output/hmm/transition_probabilities.png')\n",
    "fig.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot transitions between decoded states as a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_colors(n):\n",
    "    return [ cmx.rainbow(float(i)/n) for i in range(n) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(300, 25))\n",
    "fig.suptitle('Decoded Hidden State Sequences w/ ' + str(n_components) + ' Components')\n",
    "\n",
    "component_colors = get_n_colors(n_components)\n",
    "num_plots = len(hmm_models)+1\n",
    "subplot_idx = 1\n",
    "\n",
    "fig.add_subplot(num_plots, 1, subplot_idx)\n",
    "for i in range(0, n_components):\n",
    "    plt.axvline(x=i, linewidth=1000, color=component_colors[i])\n",
    "    x+=1\n",
    "plt.title(\"Color Legend for each Hidden State\")\n",
    "plt.yticks([])\n",
    "subplot_idx += 1\n",
    "\n",
    "for (k, hmm_model) in zip(all_subjects_all_trials_connectome_upper_triangular_flattened, hmm_models):\n",
    "    \n",
    "    print(k)\n",
    "    decoded_state_sequence = hmm_model.decode(all_subjects_all_trials_connectome_upper_triangular_flattened[k])\n",
    "    \n",
    "    fig.add_subplot(num_plots, 1, subplot_idx)\n",
    "    x = 0\n",
    "    for state in decoded_state_sequence[1][:6000]:\n",
    "        plt.axvline(x=x, color=component_colors[state])\n",
    "        x+=1\n",
    "        \n",
    "    plt.title(\"{0} | Log probability of the produced state sequence: {1:.2f}\".format(k, decoded_state_sequence[0]))\n",
    "    plt.yticks([])\n",
    "    subplot_idx += 1    \n",
    "\n",
    "print(\"Rendering...\")\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.savefig('output/hmm/decoded_hidden_state_sequence.png')\n",
    "fig.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: How aligned are the state changes between modalities? Cramer's V test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize fMRI/EEG Connectome Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subject_id in ALL_SUBJECT_IDS:\n",
    "#     for trial_id in ALL_TRIAL_IDS:\n",
    "        \n",
    "#         # Attempt to load all connectome types\n",
    "#         connectomes = load_all_connectome_types(subject_id, trial_id,\n",
    "#                                                atlas='desikan', \n",
    "#                                                seconds_used_to_compute_fmri_connectome=60,\n",
    "#                                                exclude_bad_fmri_frames=True,\n",
    "#                                                filter_artifact_timepoints=True)\n",
    "        \n",
    "#         if connectomes is None:\n",
    "#             continue\n",
    "\n",
    "#         # Plot connectomes through time\n",
    "#         for t in range(0, connectomes['fmri'].shape[0]):\n",
    "            \n",
    "#             # Create figure and set title\n",
    "#             fig = plt.figure(figsize=(30, 35))\n",
    "#             fig.suptitle('Subject: \"{0}\" | Trial: {1} | Time: {2}'.format(subject_id, trial_id, t), fontsize=50)\n",
    "            \n",
    "#             # Plot connectomes\n",
    "#             subplot_idx = 1\n",
    "#             for connectome_id, connectome in connectomes.items():\n",
    "\n",
    "#                 ax = fig.add_subplot(len(connectomes), 2, subplot_idx)\n",
    "#                 plotting.plot_connectome(connectome[t], desikan_atlas_coordinates(), title='{0} Connectome'.format(connectome_id),\n",
    "#                                          edge_threshold='95%', node_size=20, colorbar=True, axes=ax)\n",
    "#                 subplot_idx += 1\n",
    "            \n",
    "#                 ax = fig.add_subplot(len(connectomes), 2, subplot_idx)\n",
    "#                 plotting.plot_matrix(connectome[t], vmin=-1., vmax=1., colorbar=True, axes=ax)\n",
    "#                 subplot_idx += 1\n",
    "    \n",
    "#             plt.savefig('output/connectomes_through_time/subject={0}_trial={1}_t={2}.png'.format(subject_id, trial_id, t))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
